---
date: '2024-11-18'
draft: false
title: '从Word2Vec到Seq2Seq'
description: '探索Word2Vec、Seq2Seq'
tags: ['NLP', 'AI', '机器学习', 'Word2Vec', 'Seq2Seq']
categories:  ['机器学习']
series: ['里程碑']
---


## 1. Word2Vec（2013）

### 核心思想
Word2Vec 是由 Google 在 2013 年提出的**词嵌入（word embedding）方法**，旨在将词汇映射为**低维稠密向量**，使得语义相近的词在向量空间中距离更近。

### 两种主要模型结构：
- **CBOW（Continuous Bag of Words）**：根据上下文词预测中心词。
- **Skip-gram**：根据中心词预测上下文词（更适合处理稀有词）。

### 特点与贡献：
- 首次实现**大规模、高效、自动学习词向量**；
- 向量具有**语义和语法性质**（如 `king - man + woman ≈ queen`）；
- 为后续深度学习模型（如 RNN、Transformer）提供了高质量的输入表示。

> 📌 **示例**：  
> “猫” 和 “狗” 的向量很接近，而 “猫” 和 “汽车” 距离较远。

---

## 2. Seq2Seq（Sequence-to-Sequence，2014）

### 核心思想
Seq2Seq 是一种**端到端的序列建模框架**，主要用于将一个序列（如句子）转换为另一个序列（如翻译结果）。典型结构包含两个部分：
- **编码器（Encoder）**：将输入序列压缩为一个固定长度的上下文向量（context vector）；
- **解码器（Decoder）**：基于该向量逐步生成目标序列。

通常使用 **RNN（尤其是 LSTM 或 GRU）** 实现编码器和解码器。

### 典型应用：
- 机器翻译（如英译中）
- 文本摘要
- 对话生成
- 语音识别

### 局限性：
- 上下文向量成为信息瓶颈，难以处理长序列；
- 编码器无法关注输入中特定部分。

> ✨ **改进**：2015 年提出的 **Attention 机制** 解决了上述问题，使模型能动态关注输入的不同位置，显著提升性能，并为 Transformer 奠定基础。

---

## 总结对比

| 项目 | Word2Vec | Seq2Seq |
|------|---------|--------|
| **提出时间** | 2013 | 2014 |
| **目标** | 学习词的向量表示 | 实现序列到序列的转换 |
| **核心技术** | 浅层神经网络 + 负采样 | RNN 编码器-解码器架构 |
| **影响** | 开启分布式语义表示时代 | 推动神经机器翻译与生成式 NLP 发展 |

两者共同标志着 NLP 从**传统统计方法**迈向**深度表示学习**的关键转折。